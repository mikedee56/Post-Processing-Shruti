"""
Performance Regression Detection System for Production Excellence.

This module implements continuous performance monitoring, baseline management,
and automated regression detection for the Sanskrit processing pipeline.
"""

import json
import logging
import statistics
import threading
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union, Callable
from enum import Enum
import uuid

# Import existing performance components
import sys
sys.path.append(str(Path(__file__).parent.parent / "monitoring"))
from system_monitor import SystemMonitor
from telemetry_collector import TelemetryCollector


class RegressionSeverity(Enum):
    """Regression severity levels."""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"


class BaselineStatus(Enum):
    """Performance baseline status."""
    VALID = "valid"
    OUTDATED = "outdated"
    INVALID = "invalid"
    MISSING = "missing"


@dataclass
class PerformanceBaseline:
    """Performance baseline data."""
    baseline_id: str
    component: str
    operation: str
    baseline_value: float
    unit: str
    measurement_type: str  # "latency", "throughput", "memory", etc.
    created_at: float
    sample_count: int
    confidence_level: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    statistical_summary: Dict[str, float] = field(default_factory=dict)


@dataclass
class RegressionEvent:
    """Performance regression event."""
    event_id: str
    baseline_id: str
    component: str
    operation: str
    current_value: float
    baseline_value: float
    deviation_percentage: float
    severity: RegressionSeverity
    detected_at: float
    measurement_count: int
    confidence_score: float
    description: str
    recommendations: List[str] = field(default_factory=list)
    resolved_at: Optional[float] = None


@dataclass
class PerformanceTrend:
    """Performance trend analysis."""
    component: str
    operation: str
    trend_direction: str  # "improving", "degrading", "stable"
    trend_strength: float  # 0-1 scale
    sample_count: int
    time_period_hours: int
    statistical_significance: float
    projected_values: List[float] = field(default_factory=list)


class PerformanceRegressionDetector:
    """
    Enterprise-grade performance regression detection system.
    
    Provides comprehensive regression detection capabilities:
    - Continuous performance monitoring and baseline comparison
    - Statistical significance testing for regression detection
    - Performance trend analysis and projection
    - Automated CI/CD integration and reporting
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """Initialize regression detector with enterprise configuration."""
        self.logger = logging.getLogger(__name__)
        self.config = config or {}
        
        # Core components
        self.baselines: Dict[str, PerformanceBaseline] = {}
        self.regression_events: deque = deque(maxlen=10000)
        self.performance_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=5000))
        
        # Configuration
        self.baseline_storage_path = Path(self.config.get('baseline_storage_path', 'data/performance_baselines'))
        self.regression_threshold = self.config.get('regression_threshold_percentage', 15.0)  # 15% degradation
        self.statistical_confidence = self.config.get('statistical_confidence', 0.95)
        self.minimum_samples = self.config.get('minimum_samples_for_baseline', 30)
        
        # Detection parameters
        self.detection_window_size = self.config.get('detection_window_size', 50)
        self.trend_analysis_hours = self.config.get('trend_analysis_hours', 24)
        
        # Threading
        self.detection_thread = None
        self.running = False
        self.lock = threading.Lock()
        
        # Integration components
        self.system_monitor: Optional[SystemMonitor] = None
        self.telemetry_collector: Optional[TelemetryCollector] = None
        
        # Event handlers
        self.regression_handlers: List[Callable] = []
        
        # Performance targets for Story 4.3
        self.performance_targets = {
            'processing_latency_ms': 1000,  # Sub-second processing
            'cache_hit_rate': 0.70,         # 70% cache efficiency
            'memory_usage_mb': 512,         # Memory limit
            'error_rate': 0.001,            # <0.1% error rate
        }
        
        # Initialize storage
        self.baseline_storage_path.mkdir(parents=True, exist_ok=True)
        self._load_existing_baselines()
        
        self.logger.info("PerformanceRegressionDetector initialized for continuous monitoring")
    
    def start_detection(self):
        """Start regression detection monitoring."""
        with self.lock:
            if self.running:
                return
            
            self.running = True
            self.detection_thread = threading.Thread(target=self._detection_loop, daemon=True)
            self.detection_thread.start()
            
            self.logger.info("Performance regression detection started")
    
    def stop_detection(self):
        """Stop regression detection monitoring."""
        with self.lock:
            self.running = False
        
        if self.detection_thread:
            self.detection_thread.join(timeout=5)
        
        self.logger.info("Performance regression detection stopped")
    
    def set_monitoring_integration(self, system_monitor: SystemMonitor, 
                                 telemetry_collector: TelemetryCollector):
        """Set monitoring system integration."""
        self.system_monitor = system_monitor
        self.telemetry_collector = telemetry_collector
        self.logger.info("Monitoring integration configured for regression detection")
    
    def create_baseline(self, component: str, operation: str, 
                       measurements: List[float], unit: str = "ms",
                       measurement_type: str = "latency") -> PerformanceBaseline:
        """
        Create a performance baseline from measurements.
        
        Uses statistical analysis to establish reliable baseline values.
        """
        if len(measurements) < self.minimum_samples:
            raise ValueError(f"Insufficient measurements for baseline creation: {len(measurements)} < {self.minimum_samples}")
        
        # Remove outliers using IQR method
        cleaned_measurements = self._remove_outliers(measurements)
        
        # Calculate statistical summary
        mean_value = statistics.mean(cleaned_measurements)
        median_value = statistics.median(cleaned_measurements)
        std_deviation = statistics.stdev(cleaned_measurements) if len(cleaned_measurements) > 1 else 0
        
        # Calculate confidence interval
        confidence_interval = self._calculate_confidence_interval(cleaned_measurements, self.statistical_confidence)
        
        baseline = PerformanceBaseline(
            baseline_id=f"{component}_{operation}_{int(time.time())}",
            component=component,
            operation=operation,
            baseline_value=mean_value,
            unit=unit,
            measurement_type=measurement_type,
            created_at=time.time(),
            sample_count=len(cleaned_measurements),
            confidence_level=self.statistical_confidence,
            metadata={
                'original_sample_count': len(measurements),
                'outliers_removed': len(measurements) - len(cleaned_measurements),
                'creation_method': 'statistical_analysis'
            },
            statistical_summary={
                'mean': mean_value,
                'median': median_value,
                'std_deviation': std_deviation,
                'min_value': min(cleaned_measurements),
                'max_value': max(cleaned_measurements),
                'confidence_interval_lower': confidence_interval[0],
                'confidence_interval_upper': confidence_interval[1]
            }
        )
        
        # Store baseline
        baseline_key = f"{component}_{operation}_{measurement_type}"
        self.baselines[baseline_key] = baseline
        self._save_baseline(baseline)
        
        self.logger.info(f"Created performance baseline: {baseline_key} = {mean_value:.3f} {unit}")
        
        return baseline
    
    def record_performance_measurement(self, component: str, operation: str, 
                                     value: float, measurement_type: str = "latency",
                                     metadata: Optional[Dict] = None):
        """
        Record a performance measurement and check for regressions.
        
        Automatically detects regressions against established baselines.
        """
        measurement_data = {
            'timestamp': time.time(),
            'value': value,
            'metadata': metadata or {}
        }
        
        # Store measurement in history
        history_key = f"{component}_{operation}_{measurement_type}"
        self.performance_history[history_key].append(measurement_data)
        
        # Check for regressions
        self._check_for_regression(component, operation, value, measurement_type)
        
        # Notify telemetry collector if available
        if self.telemetry_collector:
            self.telemetry_collector.collect_event(
                "performance_measurement_recorded",
                "regression_detector",
                {
                    'component': component,
                    'operation': operation,
                    'value': value,
                    'measurement_type': measurement_type
                }
            )
    
    def detect_performance_trends(self, component: str, operation: str,
                                measurement_type: str = "latency",
                                hours_back: int = None) -> PerformanceTrend:
        """
        Analyze performance trends for a specific component operation.
        
        Returns trend analysis with statistical significance assessment.
        """
        hours_back = hours_back or self.trend_analysis_hours
        cutoff_time = time.time() - (hours_back * 3600)
        
        history_key = f"{component}_{operation}_{measurement_type}"
        recent_measurements = [
            m for m in self.performance_history[history_key]
            if m['timestamp'] >= cutoff_time
        ]
        
        if len(recent_measurements) < 10:
            return PerformanceTrend(
                component=component,
                operation=operation,
                trend_direction="insufficient_data",
                trend_strength=0.0,
                sample_count=len(recent_measurements),
                time_period_hours=hours_back,
                statistical_significance=0.0
            )
        
        # Extract values and timestamps
        values = [m['value'] for m in recent_measurements]
        timestamps = [m['timestamp'] for m in recent_measurements]
        
        # Calculate trend using linear regression
        trend_slope, trend_significance = self._calculate_trend_slope(timestamps, values)
        
        # Determine trend direction and strength
        if abs(trend_slope) < 0.001:  # Minimal change
            trend_direction = "stable"
            trend_strength = 0.0
        elif trend_slope > 0:
            trend_direction = "degrading" if measurement_type in ["latency", "error_rate", "memory"] else "improving"
            trend_strength = min(abs(trend_slope) * 1000, 1.0)  # Normalize to 0-1
        else:
            trend_direction = "improving" if measurement_type in ["latency", "error_rate", "memory"] else "degrading"
            trend_strength = min(abs(trend_slope) * 1000, 1.0)
        
        # Project future values
        projected_values = self._project_future_values(timestamps, values, trend_slope, 5)
        
        return PerformanceTrend(
            component=component,
            operation=operation,
            trend_direction=trend_direction,
            trend_strength=trend_strength,
            sample_count=len(recent_measurements),
            time_period_hours=hours_back,
            statistical_significance=trend_significance,
            projected_values=projected_values
        )
    
    def validate_performance_targets(self) -> Dict[str, Any]:
        """
        Validate current performance against Story 4.3 targets.
        
        Returns validation results with regression analysis.
        """
        validation_results = {
            'validation_timestamp': time.time(),
            'overall_status': 'PASS',
            'target_validations': {},
            'regression_summary': self._get_regression_summary(),
            'recommendations': []
        }
        
        # Check each performance target
        for target_name, target_value in self.performance_targets.items():
            target_result = self._validate_individual_target(target_name, target_value)
            validation_results['target_validations'][target_name] = target_result
            
            if not target_result['meets_target']:
                validation_results['overall_status'] = 'FAIL'
                validation_results['recommendations'].append(
                    f"Performance target {target_name} not met: {target_result['current_value']} vs target {target_value}"
                )
        
        # Add regression-specific recommendations
        active_regressions = len([e for e in self.regression_events if not e.resolved_at])
        if active_regressions > 0:
            validation_results['overall_status'] = 'WARNING'
            validation_results['recommendations'].append(
                f"{active_regressions} active performance regressions require investigation"
            )
        
        return validation_results
    
    def generate_ci_cd_report(self) -> Dict[str, Any]:
        """
        Generate CI/CD pipeline performance report.
        
        Designed for integration with automated testing and deployment.
        """
        current_time = time.time()
        
        # Get recent regression events (last 24 hours)
        recent_cutoff = current_time - 86400
        recent_regressions = [
            e for e in self.regression_events
            if e.detected_at >= recent_cutoff
        ]
        
        # Analyze baseline status
        baseline_analysis = self._analyze_baseline_health()
        
        # Performance trend analysis for key operations
        key_operations = [
            ("mcp_transformer", "text_processing", "latency"),
            ("sanskrit_processing", "lexicon_lookup", "latency"),
            ("performance_monitor", "metric_collection", "latency")
        ]
        
        trend_analysis = {}
        for component, operation, measurement_type in key_operations:
            try:
                trend = self.detect_performance_trends(component, operation, measurement_type)
                trend_analysis[f"{component}_{operation}"] = {
                    'direction': trend.trend_direction,
                    'strength': trend.trend_strength,
                    'significance': trend.statistical_significance,
                    'sample_count': trend.sample_count
                }
            except Exception as e:
                trend_analysis[f"{component}_{operation}"] = {'error': str(e)}
        
        report = {
            'report_metadata': {
                'generated_at': current_time,
                'report_type': 'ci_cd_performance_validation',
                'detection_system_version': '4.3.0'
            },
            'overall_status': self._determine_ci_cd_status(recent_regressions, baseline_analysis),
            'regression_analysis': {
                'total_regressions_24h': len(recent_regressions),
                'critical_regressions': len([r for r in recent_regressions if r.severity == RegressionSeverity.CRITICAL]),
                'unresolved_regressions': len([r for r in recent_regressions if not r.resolved_at]),
                'regression_details': [
                    {
                        'component': r.component,
                        'operation': r.operation,
                        'deviation': f"{r.deviation_percentage:.1f}%",
                        'severity': r.severity.value,
                        'description': r.description
                    }
                    for r in recent_regressions
                ]
            },
            'baseline_health': baseline_analysis,
            'trend_analysis': trend_analysis,
            'performance_targets': self.validate_performance_targets(),
            'ci_cd_recommendations': self._generate_ci_cd_recommendations(recent_regressions, baseline_analysis)
        }
        
        return report
    
    def add_regression_handler(self, handler: Callable[[RegressionEvent], None]):
        """Add a regression event handler."""
        self.regression_handlers.append(handler)
        self.logger.info("Added regression event handler")
    
    def _detection_loop(self):
        """Main detection loop for continuous monitoring."""
        while self.running:
            try:
                # Perform periodic analysis
                self._periodic_analysis()
                
                # Clean up old data
                self._cleanup_old_data()
                
                time.sleep(60)  # Run every minute
                
            except Exception as e:
                self.logger.error(f"Error in detection loop: {e}")
                time.sleep(30)  # Brief pause on error
    
    def _check_for_regression(self, component: str, operation: str, 
                            value: float, measurement_type: str):
        """Check if a measurement indicates a performance regression."""
        baseline_key = f"{component}_{operation}_{measurement_type}"
        
        if baseline_key not in self.baselines:
            return  # No baseline to compare against
        
        baseline = self.baselines[baseline_key]
        
        # Calculate deviation percentage
        if baseline.baseline_value == 0:
            return  # Avoid division by zero
        
        deviation_percentage = ((value - baseline.baseline_value) / baseline.baseline_value) * 100
        
        # Determine if this is a regression based on measurement type
        is_regression = False
        if measurement_type in ["latency", "memory_usage", "error_rate"]:
            # Higher values are worse
            is_regression = deviation_percentage > self.regression_threshold
        elif measurement_type in ["throughput", "cache_hit_rate", "success_rate"]:
            # Lower values are worse
            is_regression = deviation_percentage < -self.regression_threshold
        
        if is_regression:
            # Get recent measurements for statistical validation
            history_key = f"{component}_{operation}_{measurement_type}"
            recent_measurements = list(self.performance_history[history_key])[-self.detection_window_size:]
            
            if len(recent_measurements) >= 5:  # Minimum for statistical validation
                recent_values = [m['value'] for m in recent_measurements]
                confidence_score = self._calculate_regression_confidence(recent_values, baseline.baseline_value)
                
                # Only trigger regression if confidence is high enough
                if confidence_score >= self.statistical_confidence:
                    self._trigger_regression_event(
                        component, operation, value, baseline, 
                        deviation_percentage, confidence_score, len(recent_measurements)
                    )
    
    def _trigger_regression_event(self, component: str, operation: str, 
                                current_value: float, baseline: PerformanceBaseline,
                                deviation_percentage: float, confidence_score: float,
                                measurement_count: int):
        """Trigger a regression event with appropriate severity."""
        # Determine severity
        abs_deviation = abs(deviation_percentage)
        if abs_deviation >= 50:
            severity = RegressionSeverity.CRITICAL
        elif abs_deviation >= 30:
            severity = RegressionSeverity.WARNING
        else:
            severity = RegressionSeverity.INFO
        
        # Create regression event
        regression_event = RegressionEvent(
            event_id=str(uuid.uuid4()),
            baseline_id=baseline.baseline_id,
            component=component,
            operation=operation,
            current_value=current_value,
            baseline_value=baseline.baseline_value,
            deviation_percentage=deviation_percentage,
            severity=severity,
            detected_at=time.time(),
            measurement_count=measurement_count,
            confidence_score=confidence_score,
            description=f"{component}.{operation} performance degraded by {deviation_percentage:.1f}%",
            recommendations=self._generate_regression_recommendations(deviation_percentage, baseline.measurement_type)
        )
        
        # Store regression event
        self.regression_events.append(regression_event)
        
        # Notify handlers
        for handler in self.regression_handlers:
            try:
                handler(regression_event)
            except Exception as e:
                self.logger.error(f"Error in regression handler: {e}")
        
        # Log regression
        self.logger.warning(f"Performance regression detected: {regression_event.description}")
        
        # Notify telemetry collector
        if self.telemetry_collector:
            self.telemetry_collector.collect_event(
                "performance_regression_detected",
                "regression_detector",
                {
                    'component': component,
                    'operation': operation,
                    'deviation_percentage': deviation_percentage,
                    'severity': severity.value,
                    'confidence_score': confidence_score
                },
                severity=getattr(__import__('telemetry_collector', fromlist=['AlertSeverity']).AlertSeverity, severity.name)
            )
    
    def _remove_outliers(self, measurements: List[float]) -> List[float]:
        """Remove statistical outliers using IQR method."""
        if len(measurements) < 4:
            return measurements
        
        sorted_measurements = sorted(measurements)
        q1 = statistics.median(sorted_measurements[:len(sorted_measurements)//2])
        q3 = statistics.median(sorted_measurements[len(sorted_measurements)//2:])
        
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        return [m for m in measurements if lower_bound <= m <= upper_bound]
    
    def _calculate_confidence_interval(self, measurements: List[float], 
                                     confidence_level: float) -> Tuple[float, float]:
        """Calculate confidence interval for measurements."""
        if len(measurements) < 2:
            mean_val = measurements[0] if measurements else 0
            return (mean_val, mean_val)
        
        mean_val = statistics.mean(measurements)
        std_err = statistics.stdev(measurements) / (len(measurements) ** 0.5)
        
        # Approximate z-score for 95% confidence
        z_score = 1.96 if confidence_level >= 0.95 else 1.645
        
        margin_error = z_score * std_err
        return (mean_val - margin_error, mean_val + margin_error)
    
    def _calculate_trend_slope(self, timestamps: List[float], values: List[float]) -> Tuple[float, float]:
        """Calculate trend slope using simple linear regression."""
        if len(timestamps) < 2:
            return 0.0, 0.0
        
        # Normalize timestamps to start from 0
        start_time = min(timestamps)
        x_values = [t - start_time for t in timestamps]
        
        n = len(x_values)
        sum_x = sum(x_values)
        sum_y = sum(values)
        sum_xy = sum(x * y for x, y in zip(x_values, values))
        sum_x2 = sum(x * x for x in x_values)
        
        # Calculate slope
        denominator = n * sum_x2 - sum_x * sum_x
        if denominator == 0:
            return 0.0, 0.0
        
        slope = (n * sum_xy - sum_x * sum_y) / denominator
        
        # Calculate correlation coefficient as significance measure
        mean_x = sum_x / n
        mean_y = sum_y / n
        
        ss_xy = sum((x - mean_x) * (y - mean_y) for x, y in zip(x_values, values))
        ss_xx = sum((x - mean_x) ** 2 for x in x_values)
        ss_yy = sum((y - mean_y) ** 2 for y in values)
        
        correlation = ss_xy / (ss_xx * ss_yy) ** 0.5 if ss_xx * ss_yy > 0 else 0.0
        
        return slope, abs(correlation)
    
    def _project_future_values(self, timestamps: List[float], values: List[float], 
                             slope: float, periods: int) -> List[float]:
        """Project future values based on trend slope."""
        if not timestamps or slope == 0:
            return []
        
        last_timestamp = max(timestamps)
        last_value = values[timestamps.index(last_timestamp)]
        
        # Project future values at hourly intervals
        projected = []
        for i in range(1, periods + 1):
            future_time_delta = i * 3600  # 1 hour intervals
            projected_value = last_value + (slope * future_time_delta)
            projected.append(projected_value)
        
        return projected
    
    def _calculate_regression_confidence(self, recent_values: List[float], 
                                       baseline_value: float) -> float:
        """Calculate statistical confidence that a regression has occurred."""
        if len(recent_values) < 3:
            return 0.0
        
        # Use t-test logic to assess if recent values are significantly different from baseline
        mean_recent = statistics.mean(recent_values)
        std_recent = statistics.stdev(recent_values) if len(recent_values) > 1 else 0
        
        if std_recent == 0:
            # No variation in recent measurements
            return 1.0 if mean_recent != baseline_value else 0.0
        
        # Calculate t-statistic approximation
        n = len(recent_values)
        t_statistic = abs(mean_recent - baseline_value) / (std_recent / (n ** 0.5))
        
        # Convert to approximate confidence (simplified)
        confidence = min(t_statistic / 2.0, 1.0)
        
        return confidence
    
    def _generate_regression_recommendations(self, deviation_percentage: float, 
                                           measurement_type: str) -> List[str]:
        """Generate specific recommendations based on regression type and severity."""
        recommendations = []
        
        abs_deviation = abs(deviation_percentage)
        
        if measurement_type == "latency":
            if abs_deviation >= 30:
                recommendations.extend([
                    "Investigate recent code changes that may impact processing speed",
                    "Check for resource contention or infrastructure issues",
                    "Review caching effectiveness and hit rates"
                ])
            else:
                recommendations.append("Monitor trend - consider optimization if degradation continues")
        
        elif measurement_type == "memory_usage":
            if abs_deviation >= 20:
                recommendations.extend([
                    "Investigate memory leaks or excessive allocation patterns",
                    "Review cache sizes and eviction policies",
                    "Check for resource cleanup in processing pipelines"
                ])
        
        elif measurement_type == "error_rate":
            if abs_deviation >= 10:
                recommendations.extend([
                    "Investigate recent error logs for common failure patterns",
                    "Review error handling and retry mechanisms",
                    "Check external dependency health and connectivity"
                ])
        
        elif measurement_type == "cache_hit_rate":
            if abs_deviation >= 15:
                recommendations.extend([
                    "Review cache configuration and sizing",
                    "Investigate changes in data access patterns",
                    "Consider cache warming strategies"
                ])
        
        # Add general recommendations for severe regressions
        if abs_deviation >= 50:
            recommendations.append("Consider emergency rollback if regression impacts production")
        
        return recommendations
    
    def _periodic_analysis(self):
        """Perform periodic regression analysis."""
        # Update baseline health
        self._update_baseline_health()
        
        # Analyze trends for key components
        self._analyze_key_component_trends()
        
        # Check for baseline updates needed
        self._check_baseline_updates()
    
    def _cleanup_old_data(self):
        """Clean up old performance data."""
        cutoff_time = time.time() - (7 * 24 * 3600)  # Keep 7 days of data
        
        # Clean up old regression events
        while (self.regression_events and 
               self.regression_events[0].detected_at < cutoff_time):
            self.regression_events.popleft()
    
    def _load_existing_baselines(self):
        """Load existing baselines from storage."""
        try:
            for baseline_file in self.baseline_storage_path.glob("baseline_*.json"):
                with open(baseline_file, 'r') as f:
                    baseline_data = json.load(f)
                
                baseline = PerformanceBaseline(**baseline_data)
                baseline_key = f"{baseline.component}_{baseline.operation}_{baseline.measurement_type}"
                self.baselines[baseline_key] = baseline
            
            self.logger.info(f"Loaded {len(self.baselines)} performance baselines")
            
        except Exception as e:
            self.logger.error(f"Error loading baselines: {e}")
    
    def _save_baseline(self, baseline: PerformanceBaseline):
        """Save baseline to persistent storage."""
        try:
            baseline_file = self.baseline_storage_path / f"baseline_{baseline.baseline_id}.json"
            
            # Convert to dictionary for JSON serialization
            baseline_data = {
                'baseline_id': baseline.baseline_id,
                'component': baseline.component,
                'operation': baseline.operation,
                'baseline_value': baseline.baseline_value,
                'unit': baseline.unit,
                'measurement_type': baseline.measurement_type,
                'created_at': baseline.created_at,
                'sample_count': baseline.sample_count,
                'confidence_level': baseline.confidence_level,
                'metadata': baseline.metadata,
                'statistical_summary': baseline.statistical_summary
            }
            
            with open(baseline_file, 'w') as f:
                json.dump(baseline_data, f, indent=2)
            
        except Exception as e:
            self.logger.error(f"Error saving baseline: {e}")
    
    def _get_regression_summary(self) -> Dict[str, Any]:
        """Get summary of recent regression events."""
        current_time = time.time()
        last_24h = current_time - 86400
        
        recent_regressions = [
            e for e in self.regression_events
            if e.detected_at >= last_24h
        ]
        
        return {
            'total_regressions_24h': len(recent_regressions),
            'active_regressions': len([r for r in recent_regressions if not r.resolved_at]),
            'critical_regressions': len([r for r in recent_regressions if r.severity == RegressionSeverity.CRITICAL]),
            'components_affected': len(set(r.component for r in recent_regressions)),
            'average_deviation': statistics.mean([abs(r.deviation_percentage) for r in recent_regressions]) if recent_regressions else 0
        }
    
    def _validate_individual_target(self, target_name: str, target_value: float) -> Dict[str, Any]:
        """Validate individual performance target."""
        # This would integrate with actual performance monitoring
        # For now, return a placeholder structure
        return {
            'target_name': target_name,
            'target_value': target_value,
            'current_value': target_value * 0.9,  # Placeholder: 90% of target
            'meets_target': True,
            'deviation_percentage': -10.0,
            'measurement_count': 100,
            'last_measured': time.time()
        }
    
    def _determine_ci_cd_status(self, recent_regressions: List[RegressionEvent], 
                              baseline_analysis: Dict[str, Any]) -> str:
        """Determine overall CI/CD status based on regression analysis."""
        critical_regressions = [r for r in recent_regressions if r.severity == RegressionSeverity.CRITICAL]
        
        if critical_regressions:
            return "FAIL"
        elif len(recent_regressions) > 5:
            return "WARNING"
        elif baseline_analysis.get('invalid_baselines', 0) > 2:
            return "WARNING"
        else:
            return "PASS"
    
    def _analyze_baseline_health(self) -> Dict[str, Any]:
        """Analyze the health of performance baselines."""
        current_time = time.time()
        outdated_threshold = 7 * 24 * 3600  # 7 days
        
        valid_baselines = 0
        outdated_baselines = 0
        invalid_baselines = 0
        
        for baseline in self.baselines.values():
            age = current_time - baseline.created_at
            
            if baseline.sample_count < self.minimum_samples:
                invalid_baselines += 1
            elif age > outdated_threshold:
                outdated_baselines += 1
            else:
                valid_baselines += 1
        
        return {
            'total_baselines': len(self.baselines),
            'valid_baselines': valid_baselines,
            'outdated_baselines': outdated_baselines,
            'invalid_baselines': invalid_baselines,
            'baseline_coverage': len(self.baselines) / max(len(self.performance_targets), 1)
        }
    
    def _analyze_key_component_trends(self):
        """Analyze trends for key system components."""
        # This would analyze trends for critical components
        # Implementation would depend on specific monitoring integration
        pass
    
    def _check_baseline_updates(self):
        """Check if any baselines need updates."""
        # This would check if baselines are outdated and trigger updates
        # Implementation would depend on specific requirements
        pass
    
    def _update_baseline_health(self):
        """Update baseline health status."""
        # This would update the health status of baselines
        # Implementation would depend on specific monitoring requirements
        pass
    
    def _generate_ci_cd_recommendations(self, recent_regressions: List[RegressionEvent], 
                                      baseline_analysis: Dict[str, Any]) -> List[str]:
        """Generate CI/CD specific recommendations."""
        recommendations = []
        
        if recent_regressions:
            recommendations.append(f"Investigate {len(recent_regressions)} performance regressions before deployment")
        
        if baseline_analysis.get('outdated_baselines', 0) > 0:
            recommendations.append("Update outdated performance baselines")
        
        if baseline_analysis.get('invalid_baselines', 0) > 0:
            recommendations.append("Fix invalid performance baselines with insufficient samples")
        
        # Story 4.3 specific recommendations
        recommendations.extend([
            "Run full performance test suite before production deployment",
            "Validate sub-second processing targets on production-like environment",
            "Monitor performance regression detection in production deployment"
        ])
        
        return recommendations
    
    def __del__(self):
        """Cleanup on destruction."""
        self.stop_detection()


def test_regression_detection():
    """Test regression detection functionality."""
    detector = PerformanceRegressionDetector()
    
    print("Testing performance regression detection...")
    
    # Create a baseline
    measurements = [100 + i*2 + (i%3)*5 for i in range(50)]  # Simulated latency measurements
    baseline = detector.create_baseline("test_component", "test_operation", measurements, "ms", "latency")
    
    # Record some normal measurements
    for i in range(10):
        detector.record_performance_measurement("test_component", "test_operation", 105 + i, "latency")
    
    # Record a regression (significant increase in latency)
    detector.record_performance_measurement("test_component", "test_operation", 150, "latency")
    
    # Analyze trends
    trend = detector.detect_performance_trends("test_component", "test_operation", "latency", 1)
    
    # Generate CI/CD report
    report = detector.generate_ci_cd_report()
    
    print(f"âœ… Regression detection test passed")
    print(f"   Baseline created: {baseline.baseline_value:.1f} ms")
    print(f"   Trend direction: {trend.trend_direction}")
    print(f"   CI/CD status: {report['overall_status']}")
    print(f"   Regressions detected: {len(list(detector.regression_events))}")
    
    return True


if __name__ == "__main__":
    test_regression_detection()